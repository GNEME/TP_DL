{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4a2b97-c57c-4e56-ac7c-1f99420d760c",
   "metadata": {},
   "source": [
    "**Partie 1**\n",
    "\n",
    "$1.1 Jeu \\ de \\ données$\n",
    "\n",
    "$1-$ Les données d'apprentissages servent à entraîner le modèle. Tandis que les données de validation utilisées au cours de l'entrainement, elles permettent de régler les hyper paramètres du modèle. Quant aux données de test, elles permettent de s'assurer de la capacité du modèle à pouvoir généraliser ses prédictions sur de nouvelles données.\n",
    "\n",
    "$2-$ Plus N est grand, plus le modèle apprend à mieux généraliser et évite l'overfitting. Cela peut cependant nécessiter une plus rande puissance de calcul au niveau du matériel et un temps d'entrainement plus long.\n",
    "\n",
    "$3-$ L'utilisation des fonctions d'activation permet d'une part d'introduire la non linéarité (pour les fonctions d'activation non linéaires) et d'apprendre des relations non linéaire complexes, d'autre part \n",
    "\n",
    "$4-$ nx : représente le nombre de caractéristiques d'un individu de l'échantillon, nh: le nombre de neurones dans la couche cachée, ny : le nombre de sortie. En pratique, nx est choisi en fonction du nombre de caractéristiques présentes chez un individu du dataset, ny est choisi est choisi en fonction du nombre de classes à classifier, nh de manière aléatoire.\n",
    "\n",
    "$5-$ yˆ représente les prédictions du modèle. Par contre y lui représente les vraies valeurs.\n",
    "\n",
    "$6-$ On utilise une fonction softmax pour normaliser les sorties de chaque neurone présent dans la couche de sortie afin d'avoir une somme des probabilités égale à 1.\n",
    "\n",
    "$7-$\n",
    "\n",
    "$8-$\n",
    "\n",
    "$9-$\n",
    "\n",
    "$10-$\n",
    "\n",
    "$11-$\n",
    "\n",
    "$12-$\n",
    "\n",
    "$13-$\n",
    "\n",
    "$14-$\n",
    "\n",
    "$15-$\n",
    "\n",
    "$16-$\n",
    "\n",
    "$17-$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17137b2e-c929-480a-9e83-d4d90fc78f7d",
   "metadata": {},
   "source": [
    "**Partie 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4221d7d2-fe07-4f0d-be83-f97196682639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd4a1adf-b96b-4281-a65d-65767f59bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0466,  0.5971],\n",
      "        [ 0.3085, -1.2135]])\n"
     ]
    }
   ],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    weights = torch.randn((nh, nx))\n",
    "    print(weights)\n",
    "\n",
    "init_params(2, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "88dfccfe-c375-4bfc-87be-9e1daaec0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TP1_Model(nn.Module):\n",
    "    def __init__(self, nx, nh, ny):\n",
    "        super().__init__()\n",
    "        self.nx = nx\n",
    "        self.nh = nh\n",
    "        self.ny = ny\n",
    "        self.params = self.init_params(nx, nh, ny)\n",
    "    def init_params(self, nx, nh, ny):\n",
    "        pass\n",
    "    def forward(self, params, X):\n",
    "        pass\n",
    "    def loss_accuracy(Yhat, Y):\n",
    "        pass\n",
    "    def backward(params, outputs, Y):\n",
    "        \"\"\"\n",
    "        set all grads to None before calling this function\n",
    "        params : are weights\n",
    "        outputs : list that contains the gradients of loss of every batch\n",
    "        Y : equivalent to L (loss) which is the output of loss_accuracy\n",
    "        \"\"\"\n",
    "        #nn.Module.zero_grad() : set all grads to None here\n",
    "        Y.backward()\n",
    "        outputs.append(params.grad)\n",
    "    def sgd(params, grads, eta):\n",
    "        \"\"\"\n",
    "        This function will return the updated params\n",
    "        params : are weights\n",
    "        eta : pas d'apprentissage\n",
    "        grads : List of gradients\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            #params = params - eta * grads[-1]\n",
    "            params = torch.sub(params, grads[-1], alpha=eta)\n",
    "        return params.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c5974200-47d9-4667-8dcf-cf136a0f90bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = torch.tensor([1], dtype=torch.float)\\nx.requires_grad_(True)\\nloss = x @ x\\noutput = []\\nbackward(x, output, loss)\\noutput\\nprint(x)\\nprint(output)\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backward(params, outputs, Y):\n",
    "    \"\"\"\n",
    "    set all grads to None before calling this function\n",
    "    params : are weights\n",
    "    outputs : list that contains the gradients of loss of every batch\n",
    "    Y : equivalent to L (loss) which is the output of loss_accuracy\n",
    "    \"\"\"\n",
    "    #nn.Module.zero_grad() : set all grads to None here\n",
    "    Y.backward()\n",
    "    outputs.append(params.grad)\n",
    "\n",
    "# Test part\n",
    "\"\"\"\n",
    "x = torch.tensor([1], dtype=torch.float)\n",
    "x.requires_grad_(True)\n",
    "loss = x @ x\n",
    "output = []\n",
    "backward(x, output, loss)\n",
    "output\n",
    "print(x)\n",
    "print(output)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c06b7658-9605-4ddb-95ea-d92c10e36608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = torch.tensor([1], dtype=torch.float)\\nx.requires_grad_(True)\\nloss = x @ x\\noutput = []\\nbackward(x, output, loss)\\nprint(\"x = \", x)\\nprint(\"grads = \", output[-1])\\nx = sgd(x, output, 0.01)\\nprint(\"updated x = \", x)\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd(params, grads, eta):\n",
    "    \"\"\"\n",
    "    This function will return the updated params\n",
    "    params : are weights\n",
    "    eta : pas d'apprentissage\n",
    "    grads : List of gradients\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        #params = params - eta * grads[-1]\n",
    "        params = torch.sub(params, grads[-1], alpha=eta)\n",
    "    return params.requires_grad_(True)\n",
    "\n",
    "# Test part\n",
    "\"\"\"\n",
    "x = torch.tensor([1], dtype=torch.float)\n",
    "x.requires_grad_(True)\n",
    "loss = x @ x\n",
    "output = []\n",
    "backward(x, output, loss)\n",
    "print(\"x = \", x)\n",
    "print(\"grads = \", output[-1])\n",
    "x = sgd(x, output, 0.01)\n",
    "print(\"updated x = \", x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e7bbbdf3-6036-4465-a47d-f37daac120ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs=100, learning_rate=0.01):\n",
    "    \n",
    "    history = {\"loss\": [],\n",
    "              \"acc\": [],\n",
    "              }\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        outputs = []\n",
    "        losses = []\n",
    "        accs = []\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            \n",
    "            ## Step 1: Run the model on the input data\n",
    "            Yhat = model.forward(model.params, data_inputs)\n",
    "            Yhat = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            \n",
    "            ## Step 2: Calculate the loss\n",
    "            loss = loss_accuracy(Yhat, data_labels.float())\n",
    "            losses.append(loss['L'])\n",
    "            accs.append(loss[\"acc\"])\n",
    "            \n",
    "            ## Step 3: Perform backpropagation\n",
    "            model.zero_grad()\n",
    "            model.backward(model.params, outputs, data_labels.float())\n",
    "            \n",
    "            ## Step 4: Update the parameters\n",
    "            model.params = model.sgd(model.params, outputs, learning_rate)\n",
    "        history[\"loss\"].append(mean(losses))\n",
    "        history[\"acc\"].append(mean(accs))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "107f3cf2-2e4a-4cc8-a351-d55713c96bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history):\n",
    "    x = np.arange(1, len(history[\"loss\"]) + 1)\n",
    "    plt.title(\"Courbe de loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(x, history[\"loss\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9bc8e9ee-366a-4d74-80e4-0e29dd467b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_grid():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6e019-864a-42df-be4b-c1bcc098156a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
